{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME-Aleph\n",
    "\n",
    "### KI-Campus Aufgabe\n",
    "\n",
    "Willkommen zum Arbeitsauftrag für das Modul __LIME-Aleph__ im KI-Campus. Hier werden Sie den typischen Ablauf zum Finden einer symbolischen Erklärung für Black-Box Netzwerke mithilfe der LIME-Aleph Bibliothek Stück für Stück erarbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen zunächst mal die nötigen Bibliotheken importieren und einige nutzerdefinierbare Parameter erzeugen. Eine zu klassifizierende Bilddatei sowie ein vortrainiertes Modell sind schon vorhanden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import img_as_float32\n",
    "from skimage.transform import resize\n",
    "from train_model import own_rel\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "from skimage import io\n",
    "from skimage.io import imshow, show, imsave\n",
    "import shutil\n",
    "\n",
    "import lime_aleph as la\n",
    "\n",
    "\n",
    "IMAGE_FILE = \"./pos9000.png\" # The path to the image file to be classified by the black-box\n",
    "MODEL = \"../models_to_explain/model_tower.h5\" # The path to the pre-trained model\n",
    "K = 3 # The number of important superpixels to be used for perturbation\n",
    "N = 1000 # The sample size for LIME\n",
    "OUTPUT_DIR = \"../output/\" # A path for a directory to save intermediate and output data\n",
    "T = 0.8 # The threshold for the binary classifier for when an example is classified as 'positive'\n",
    "NOISE = 10 # The allowed false-positive rate for Aleph in percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sollte es noch temporäre Daten aus früheren Durchläufen geben, sollen diese nun gelöscht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir das Bild und das vortrainierte Modell in den Speicher laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = img_as_float32(io.imread(IMAGE_FILE))\n",
    "image = resize(image, (own_rel.IMAGE_SIZE, own_rel.IMAGE_SIZE), anti_aliasing=True)\n",
    "\n",
    "model = own_rel.own_rel()\n",
    "model.load_weights(MODEL)\n",
    "\n",
    "io.imshow(image)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Schritt soll nun sein, die im Bild vorhandenen Elemente automatisch zu annotieren. Benutzen Sie hierfür die Funktion __annotate_image_parts__ aus dem bereits importierten __lime_aleph__ package mit den benötigten Parametern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LIME...\n",
      "True class of the image is:  1\n",
      "Negative estimator: 0.023822417\n",
      "Positive estimator: 0.9761776\n",
      "Starting the explanation generation process. This may take several minutes. Seriously. Grab a cup of coffee.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54891b04ef7544049c0f172a19b2fb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intercept -0.20280486275716522\n",
      "Prediction_local [0.31627629]\n",
      "Right: 0.023822423\n",
      "Intercept 1.2028048627167234\n",
      "Prediction_local [0.68372371]\n",
      "Right: 0.9761776\n",
      "Elapsed time: 2.4895732402801514\n",
      "Number of superpixels: 64\n",
      "Annotating the superpixels...\n",
      "Weight of sp:  44 is:  0.12419629683933532\n",
      "Weight of sp:  60 is:  0.11501543664701494\n",
      "Weight of sp:  52 is:  0.11038034063954716\n",
      "Weight of sp:  26 is:  -0.04124681698052051\n",
      "Weight of sp:  18 is:  -0.03935497534735091\n",
      "Weight of sp:  20 is:  -0.03863107114929205\n",
      "Weight of sp:  32 is:  -0.03675748619585719\n",
      "Weight of sp:  33 is:  -0.03196233656160736\n",
      "Weight of sp:  21 is:  -0.030046817512787823\n",
      "Weight of sp:  14 is:  -0.027722160536166673\n",
      "Weight of sp:  48 is:  -0.02736746523505049\n",
      "Weight of sp:  5 is:  0.02677362057665112\n",
      "Weight of sp:  35 is:  -0.026602177835968188\n",
      "Weight of sp:  12 is:  -0.025736433444713457\n",
      "Weight of sp:  25 is:  -0.02510378547877642\n",
      "Weight of sp:  50 is:  -0.024711291494698102\n",
      "Weight of sp:  6 is:  -0.02420615925003589\n",
      "Weight of sp:  42 is:  -0.024141120619457362\n",
      "Weight of sp:  49 is:  -0.023354887111665584\n",
      "Weight of sp:  10 is:  -0.02242770997025646\n",
      "Weight of sp:  41 is:  -0.022368659208260134\n",
      "Weight of sp:  54 is:  -0.022210511862409178\n",
      "Weight of sp:  43 is:  -0.022192897075009838\n",
      "Weight of sp:  40 is:  -0.022149941417775963\n",
      "Weight of sp:  62 is:  -0.020393894464099473\n",
      "Weight of sp:  1 is:  -0.020120241725108727\n",
      "Weight of sp:  31 is:  -0.01899310948183997\n",
      "Weight of sp:  23 is:  -0.018329407877366907\n",
      "Weight of sp:  15 is:  -0.0183038343886728\n",
      "Weight of sp:  34 is:  -0.018125819663971345\n",
      "Weight of sp:  46 is:  -0.018054141010948196\n",
      "Weight of sp:  57 is:  -0.01644162418987873\n",
      "Weight of sp:  63 is:  -0.016357329183021144\n",
      "Weight of sp:  45 is:  -0.015313654826660044\n",
      "Weight of sp:  36 is:  -0.015240542223198802\n",
      "Weight of sp:  27 is:  -0.013810201097584912\n",
      "Weight of sp:  55 is:  -0.013631401098369139\n",
      "Weight of sp:  11 is:  -0.013604519155153122\n",
      "Weight of sp:  8 is:  0.01305217546451205\n",
      "Weight of sp:  61 is:  -0.012974614658115918\n",
      "Weight of sp:  2 is:  0.012686007014183718\n",
      "Weight of sp:  24 is:  -0.012547941849012425\n",
      "Weight of sp:  17 is:  -0.012088104434232184\n",
      "Weight of sp:  4 is:  -0.011382926777970856\n",
      "Weight of sp:  59 is:  -0.011366375141239603\n",
      "Weight of sp:  13 is:  -0.011277393833173367\n",
      "Weight of sp:  19 is:  -0.010977630479621695\n",
      "Weight of sp:  53 is:  -0.010299346025753983\n",
      "Weight of sp:  51 is:  -0.010032417496221758\n",
      "Weight of sp:  38 is:  -0.008728081617513154\n",
      "Weight of sp:  28 is:  0.008301655182959713\n",
      "Weight of sp:  39 is:  -0.006390425133378222\n",
      "Weight of sp:  47 is:  0.005972548730381495\n",
      "Weight of sp:  58 is:  -0.005912354934832441\n",
      "Weight of sp:  30 is:  -0.004731495865802635\n",
      "Weight of sp:  0 is:  0.004592583932895727\n",
      "Weight of sp:  3 is:  -0.004388293505208799\n",
      "Weight of sp:  16 is:  -0.004363272057296415\n",
      "Weight of sp:  9 is:  -0.003007359800277884\n",
      "Weight of sp:  29 is:  -0.002758224093703679\n",
      "Weight of sp:  37 is:  -0.0025021374748096263\n",
      "Weight of sp:  22 is:  0.0008113514611081738\n",
      "Weight of sp:  56 is:  -0.0004987321979490296\n",
      "Weight of sp:  7 is:  0.0003783860156545086\n"
     ]
    }
   ],
   "source": [
    "annotated_image = la.annotate_image_parts(image, model, OUTPUT_DIR, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Bild nun annotiert ist (als Annotation wurden auch die Gewichte von LIME für die einzelnen Elemente gefunden), können wir nun die wichtigsten __K__ Bildelemente mit der Funktion __find_important_parts__ finden. Anschließend können Sie auch die Relationen zwischen den Bildteilen mit der Funktion __find_spatial_relations__ finden lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at superpixel 44\n",
      "Currently at superpixel 60\n",
      "Currently at superpixel 52\n"
     ]
    }
   ],
   "source": [
    "important_superpixels = la.find_important_parts(annotated_image, K)\n",
    "relations = la.find_spatial_relations(important_superpixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Liste, welche von der Funktion zum Finden von Relationen zurückgegeben wurde, beinhaltet Objekte vom Typ __Relation__. Hier geben wir nun beispielhaft die Informationen der ersten Relation aus. Natürlich müssen Sie den Namen der Liste an Ihre Implementation anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bottom_of\n",
      "Start: 60\n",
      "To: 44\n"
     ]
    }
   ],
   "source": [
    "print(\"Name:\", relations[0].name)\n",
    "print(\"Start:\", relations[0].start)\n",
    "print(\"To:\", relations[0].to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Name beschreibt das Prädikat der räumlichen Relation. Die weiteren Informationen beschreiben die Indices der Start- und Zielelemente der Relation innerhalb des Bildes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir das perturbierte Datenset für LIME-Aleph generieren lassen. Benutzen Sie hierzu die Funktion __perturb_instance__ mit den erforderlichen Parametern. Lassen Sie sich auch ausgeben, wie viele Instanzen im neuen Datenset sind (Es wird eine Liste mit Instanzen zurückgegeben)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_dataset = la.perturb_instance(annotated_image, relations, model, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of perturbed instances: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of perturbed instances:\", len(perturbed_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ILP-Framework Aleph benötigt mehrere Hilfsdateien, die mit der Funktion __write_aleph_files__ erzeugt werden. Rufen Sie diese Funktion auf. Es sollen alle räumlichen Relationen verwendet werden! Zur Verfügung stehen folgende Relationen: *left_of*, *right_of*, *top_of*, *bottom_of*, *on*, *under*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the input files for Aleph...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "used_relations = None\n",
    "la.write_aleph_files(annotated_image, perturbed_dataset, used_relations, OUTPUT_DIR, NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schlussendlich muss nun der Induktionsprozess von Aleph angestoßen werden. Dieser Schritt (mit der Funktion __run_aleph__) gibt auch die gefundene Erklärung aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "The explanation was saved to '../output/explanation.txt'\n"
     ]
    }
   ],
   "source": [
    "la.run_aleph(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Erklärung in Form von Regeln kann nun im angegebenen Ordner in der Datei *explanation.txt* gefunden und interpretiert werden. Wir lesen nun diese Datei aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_class(A) :-\n",
      "   contains(B,A), contains(C,A), has_color(C,lime), on_in_ex(C,B,A).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIR + \"explanation.txt\", 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
