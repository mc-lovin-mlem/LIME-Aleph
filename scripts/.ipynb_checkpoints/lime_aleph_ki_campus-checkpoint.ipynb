{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME-Aleph\n",
    "\n",
    "### KI-Campus Aufgabe\n",
    "\n",
    "Willkommen zum Arbeitsauftrag für das Modul __LIME-Aleph__ im KI-Campus. Hier werden Sie den typischen Ablauf zum Finden einer symbolischen Erklärung für Black-Box Netzwerke mithilfe der LIME-Aleph Bibliothek Stück für Stück erarbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen zunächst mal die nötigen Bibliotheken importieren und einige nutzerdefinierbare Parameter erzeugen. Eine zu klassifizierende Bilddatei sowie ein vortrainiertes Modell sind schon vorhanden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import img_as_float32\n",
    "from skimage.transform import resize\n",
    "from train_model import own_rel\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "from skimage import io\n",
    "from skimage.io import imshow, show, imsave\n",
    "import shutil\n",
    "\n",
    "import lime_aleph as la\n",
    "\n",
    "\n",
    "IMAGE_FILE = \"./pos9000.png\" # The path to the image file to be classified by the black-box\n",
    "MODEL = \"../models_to_explain/model_tower.h5\" # The path to the pre-trained model\n",
    "K = 3 # The number of important superpixels to be used for perturbation\n",
    "N = 1000 # The sample size for LIME\n",
    "OUTPUT_DIR = \"../output/\" # A path for a directory to save intermediate and output data\n",
    "T = 0.8 # The threshold for the binary classifier for when an example is classified as 'positive'\n",
    "NOISE = 10 # The allowed false-positive rate for Aleph in percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sollte es noch temporäre Daten aus früheren Durchläufen geben, sollen diese nun gelöscht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir das Bild und das vortrainierte Modell in den Speicher laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img_as_float32(io.imread(IMAGE_FILE))\n",
    "image = resize(image, (own_rel.IMAGE_SIZE, own_rel.IMAGE_SIZE), anti_aliasing=True)\n",
    "\n",
    "model = own_rel.own_rel()\n",
    "model.load_weights(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Schritt soll nun sein, die im Bild vorhandenen Elemente automatisch zu annotieren. Benutzen Sie hierfür die Funktion __annotate_image_parts__ aus dem bereits importierten __lime_aleph__ package mit den benötigten Parametern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LIME...\n",
      "True class of the image is:  1\n",
      "Negative estimator: 0.023822417\n",
      "Positive estimator: 0.9761776\n",
      "Starting the explanation generation process. This may take several minutes. Seriously. Grab a cup of coffee.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f680f686a4b45ea8ac03a901bfdb451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intercept -0.23695185435140392\n",
      "Prediction_local [0.34700801]\n",
      "Right: 0.023822423\n",
      "Intercept 1.2369518436363118\n",
      "Prediction_local [0.652992]\n",
      "Right: 0.9761776\n",
      "Elapsed time: 2.42808198928833\n",
      "Number of superpixels: 64\n",
      "Annotating the superpixels...\n",
      "Weight of sp:  44 is:  0.1348500482752954\n",
      "Weight of sp:  60 is:  0.11877077767693489\n",
      "Weight of sp:  52 is:  0.11696517352367555\n",
      "Weight of sp:  17 is:  -0.04427005838939401\n",
      "Weight of sp:  3 is:  0.04422867868493899\n",
      "Weight of sp:  9 is:  -0.041594925867993515\n",
      "Weight of sp:  8 is:  -0.040600105907661936\n",
      "Weight of sp:  42 is:  -0.03708196338014524\n",
      "Weight of sp:  2 is:  -0.03601242130310285\n",
      "Weight of sp:  22 is:  -0.033828178885296274\n",
      "Weight of sp:  5 is:  -0.03047594530829914\n",
      "Weight of sp:  10 is:  -0.03007406318738935\n",
      "Weight of sp:  20 is:  -0.02994312160435775\n",
      "Weight of sp:  34 is:  -0.028619635367175777\n",
      "Weight of sp:  43 is:  -0.02851203100708804\n",
      "Weight of sp:  45 is:  -0.0277089644433968\n",
      "Weight of sp:  29 is:  -0.0272852240741924\n",
      "Weight of sp:  48 is:  -0.027198129241110904\n",
      "Weight of sp:  50 is:  -0.02665760240007938\n",
      "Weight of sp:  0 is:  -0.024766434026879208\n",
      "Weight of sp:  32 is:  -0.024481191747826272\n",
      "Weight of sp:  38 is:  -0.024459891793248298\n",
      "Weight of sp:  55 is:  -0.024221029105951353\n",
      "Weight of sp:  51 is:  -0.023605966025830868\n",
      "Weight of sp:  49 is:  -0.022487597252662014\n",
      "Weight of sp:  33 is:  -0.022417250012930212\n",
      "Weight of sp:  58 is:  -0.02174143150469833\n",
      "Weight of sp:  23 is:  -0.020340023580716356\n",
      "Weight of sp:  12 is:  -0.01972943915945232\n",
      "Weight of sp:  25 is:  -0.019425558850618006\n",
      "Weight of sp:  1 is:  -0.018212998754127706\n",
      "Weight of sp:  35 is:  -0.018086173691864358\n",
      "Weight of sp:  15 is:  -0.01733050766374262\n",
      "Weight of sp:  30 is:  -0.01397863762955992\n",
      "Weight of sp:  21 is:  -0.013693388687971346\n",
      "Weight of sp:  61 is:  -0.013546277435657682\n",
      "Weight of sp:  14 is:  -0.013357490160684606\n",
      "Weight of sp:  56 is:  -0.012558391379547186\n",
      "Weight of sp:  57 is:  -0.012121708190317852\n",
      "Weight of sp:  41 is:  -0.01167992189361357\n",
      "Weight of sp:  26 is:  -0.011420742080684295\n",
      "Weight of sp:  19 is:  -0.01131303531187793\n",
      "Weight of sp:  31 is:  -0.011021297418177247\n",
      "Weight of sp:  27 is:  -0.010667914752165147\n",
      "Weight of sp:  54 is:  -0.01064376515827757\n",
      "Weight of sp:  53 is:  -0.010555366952294476\n",
      "Weight of sp:  28 is:  -0.010149929691116463\n",
      "Weight of sp:  24 is:  -0.008799192889397013\n",
      "Weight of sp:  36 is:  0.008462966144539803\n",
      "Weight of sp:  40 is:  -0.007668103992970574\n",
      "Weight of sp:  39 is:  -0.006702921529993798\n",
      "Weight of sp:  16 is:  -0.006622935698443146\n",
      "Weight of sp:  63 is:  0.005841622371596208\n",
      "Weight of sp:  37 is:  -0.00579942156546278\n",
      "Weight of sp:  11 is:  -0.005718693148621299\n",
      "Weight of sp:  4 is:  0.004550896465019993\n",
      "Weight of sp:  6 is:  -0.004458545721349363\n",
      "Weight of sp:  59 is:  -0.004280012908412671\n",
      "Weight of sp:  62 is:  0.0041589066708245005\n",
      "Weight of sp:  18 is:  -0.004090968068367876\n",
      "Weight of sp:  47 is:  -0.003964935875489111\n",
      "Weight of sp:  13 is:  -0.003921799183748358\n",
      "Weight of sp:  46 is:  -0.0035244136555677855\n",
      "Weight of sp:  7 is:  0.001638761461603539\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "\n",
    "annotated_image = la.annotate_image_parts(image, model, OUTPUT_DIR, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Bild nun annotiert ist (als Annotation wurden auch die Gewichte von LIME für die einzelnen Elemente gefunden), können wir nun die wichtigsten __K__ Bildelemente mit der Funktion __find_important_parts__ finden. Anschließend können Sie auch die Relationen zwischen den Bildteilen mit der Funktion __find_spatial_relations__ finden lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at superpixel 44\n",
      "Currently at superpixel 60\n",
      "Currently at superpixel 52\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "\n",
    "important_superpixels = la.find_important_parts(annotated_image, K)\n",
    "relations = la.find_spatial_relations(important_superpixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Liste, welche von der Funktion zum Finden von Relationen zurückgegeben wurde, beinhaltet Objekte vom Typ __Relation__. Hier geben wir nun beispielhaft die Informationen der ersten Relation aus. Natürlich müssen Sie den Namen der Liste an Ihre Implementation anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bottom_of\n",
      "Start: 60\n",
      "To: 44\n"
     ]
    }
   ],
   "source": [
    "print(\"Name:\", relations[0].name)\n",
    "print(\"Start:\", relations[0].start)\n",
    "print(\"To:\", relations[0].to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Name beschreibt das Prädikat der räumlichen Relation. Die weiteren Informationen beschreiben die Indices der Start- und Zielelemente der Relation innerhalb des Bildes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir das perturbierte Datenset für LIME-Aleph generieren lassen. Benutzen Sie hierzu die Funktion __perturb_instance__ mit den erforderlichen Parametern. Lassen Sie sich auch ausgeben, wie viele Instanzen im neuen Datenset sind (Es wird eine Liste mit Instanzen zurückgegeben)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of perturbed instances: 11\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "perturbed_dataset = la.perturb_instance(annotated_image, relations, model, T)\n",
    "print(\"Number of perturbed instances:\", len(perturbed_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ILP-Framework Aleph benötigt mehrere Hilfsdateien, die mit der Funktion __write_aleph_files__ erzeugt werden. Rufen Sie diese Funktion auf. Es sollen alle räumlichen Relationen verwendet werden! Zur Verfügung stehen folgende Relationen: *left_of*, *right_of*, *top_of*, *bottom_of*, *on*, *under*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the input files for Aleph...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "used_relations = None # 'None' if you want to allow all relations, otherwise list with following possibilities: [\"left_of\", \"right_of\", \"top_of\", \"bottom_of\", \"on\", \"under\"]\n",
    "la.write_aleph_files(annotated_image, perturbed_dataset, used_relations, OUTPUT_DIR, NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schlussendlich muss nun der Induktionsprozess von Aleph angestoßen werden. Dieser Schritt (mit der Funktion __run_aleph__) gibt auch die gefundene Erklärung aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "The explanation was saved to '../output/explanation.txt'\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "la.run_aleph(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Erklärung in Form von Regeln kann nun im angegebenen Ordner in der Datei *explanation.txt* gefunden und interpretiert werden. Wir lesen nun diese Datei aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_class(A) :-\n",
      "   contains(B,A), contains(C,A), has_color(C,lime), on_in_ex(C,B,A).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIR + \"explanation.txt\", 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
