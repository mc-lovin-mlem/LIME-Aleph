{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME-Aleph\n",
    "\n",
    "### KI-Campus Aufgabe\n",
    "\n",
    "Willkommen zum Arbeitsauftrag für das Modul __LIME-Aleph__ im KI-Campus. Hier werden Sie den typischen Ablauf zum Finden einer symbolischen Erklärung für Black-Box Netzwerke mithilfe der LIME-Aleph Bibliothek Stück für Stück erarbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen zunächst mal die nötigen Bibliotheken importieren und einige nutzerdefinierbare Parameter erzeugen. Eine zu klassifizierende Bilddatei sowie ein vortrainiertes Modell sind schon vorhanden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import img_as_float32\n",
    "from skimage.transform import resize\n",
    "from train_model import own_rel\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "from skimage import io\n",
    "from skimage.io import imshow, show, imsave\n",
    "import shutil\n",
    "\n",
    "import lime_aleph as la\n",
    "\n",
    "\n",
    "IMAGE_FILE = \"./pos9000.png\" # The path to the image file to be classified by the black-box\n",
    "MODEL = \"../models_to_explain/model_tower.h5\" # The path to the pre-trained model\n",
    "K = 3 # The number of important superpixels to be used for perturbation\n",
    "N = 1000 # The sample size for LIME\n",
    "OUTPUT_DIR = \"../output/\" # A path for a directory to save intermediate and output data\n",
    "T = 0.8 # The threshold for the binary classifier for when an example is classified as 'positive'\n",
    "NOISE = 10 # The allowed false-positive rate for Aleph in percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sollte es noch temporäre Daten aus früheren Durchläufen geben, sollen diese nun gelöscht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir das Bild und das vortrainierte Modell in den Speicher laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img_as_float32(io.imread(IMAGE_FILE))\n",
    "image = resize(image, (own_rel.IMAGE_SIZE, own_rel.IMAGE_SIZE), anti_aliasing=True)\n",
    "\n",
    "model = own_rel.own_rel()\n",
    "model.load_weights(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Schritt soll nun sein, die im Bild vorhandenen Elemente automatisch zu annotieren. Benutzen Sie hierfür die Funktion __annotate_image_parts__ aus dem bereits importierten __lime_aleph__ package mit den benötigten Parametern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LIME...\n",
      "True class of the image is:  1\n",
      "Negative estimator: 0.035995044\n",
      "Positive estimator: 0.96400493\n",
      "Starting the explanation generation process. This may take several minutes. Seriously. Grab a cup of coffee.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cb10b152094d958254c5748d0450e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intercept -0.32262181026993003\n",
      "Prediction_local [0.43254344]\n",
      "Right: 0.03599505\n",
      "Intercept 1.3226218107643302\n",
      "Prediction_local [0.56745656]\n",
      "Right: 0.96400493\n",
      "Elapsed time: 2.477236747741699\n",
      "Number of superpixels: 64\n",
      "Annotating the superpixels...\n",
      "Weight of sp:  46 is:  0.1314871328453616\n",
      "Weight of sp:  62 is:  0.12241275648495563\n",
      "Weight of sp:  54 is:  0.10971656750991134\n",
      "Weight of sp:  27 is:  -0.047958463546489916\n",
      "Weight of sp:  53 is:  -0.04582137761512524\n",
      "Weight of sp:  29 is:  -0.045072860320559405\n",
      "Weight of sp:  45 is:  -0.0419788846660248\n",
      "Weight of sp:  4 is:  -0.041833046309874965\n",
      "Weight of sp:  60 is:  -0.04061809019673972\n",
      "Weight of sp:  18 is:  -0.03935211536769014\n",
      "Weight of sp:  22 is:  -0.03780455038777952\n",
      "Weight of sp:  40 is:  -0.03270487517907898\n",
      "Weight of sp:  25 is:  -0.03159121438602118\n",
      "Weight of sp:  39 is:  -0.030687222389830057\n",
      "Weight of sp:  55 is:  -0.030638919945518813\n",
      "Weight of sp:  30 is:  -0.03054792706898851\n",
      "Weight of sp:  32 is:  -0.030539832614151566\n",
      "Weight of sp:  51 is:  -0.03047798732154149\n",
      "Weight of sp:  57 is:  -0.029089174149106764\n",
      "Weight of sp:  14 is:  -0.028178783794783866\n",
      "Weight of sp:  63 is:  -0.02767766694749401\n",
      "Weight of sp:  61 is:  -0.026832838524445707\n",
      "Weight of sp:  48 is:  -0.02644039767118931\n",
      "Weight of sp:  41 is:  -0.025666917736674035\n",
      "Weight of sp:  21 is:  -0.025127303301645466\n",
      "Weight of sp:  49 is:  -0.024930257912516487\n",
      "Weight of sp:  20 is:  -0.024075122190293407\n",
      "Weight of sp:  58 is:  -0.023392638605212276\n",
      "Weight of sp:  5 is:  -0.021963855827524484\n",
      "Weight of sp:  13 is:  -0.021834018459984606\n",
      "Weight of sp:  43 is:  -0.021413722180201662\n",
      "Weight of sp:  8 is:  -0.021337216385565457\n",
      "Weight of sp:  1 is:  -0.020421334683597303\n",
      "Weight of sp:  19 is:  -0.019957046770067217\n",
      "Weight of sp:  24 is:  -0.018419792422145937\n",
      "Weight of sp:  52 is:  -0.015617100185255901\n",
      "Weight of sp:  38 is:  0.015434381891096357\n",
      "Weight of sp:  26 is:  -0.014498967286582827\n",
      "Weight of sp:  11 is:  0.01438359127703202\n",
      "Weight of sp:  50 is:  -0.014170805413140744\n",
      "Weight of sp:  23 is:  -0.013656458201459448\n",
      "Weight of sp:  10 is:  -0.013544203105302069\n",
      "Weight of sp:  42 is:  -0.012213975818850187\n",
      "Weight of sp:  56 is:  -0.011272972194288078\n",
      "Weight of sp:  2 is:  -0.011109961554648816\n",
      "Weight of sp:  47 is:  -0.010975195575838812\n",
      "Weight of sp:  35 is:  -0.010797565296213694\n",
      "Weight of sp:  6 is:  -0.010231581241555135\n",
      "Weight of sp:  12 is:  0.009877399746109055\n",
      "Weight of sp:  59 is:  -0.009608328381270492\n",
      "Weight of sp:  37 is:  0.008184846991930853\n",
      "Weight of sp:  7 is:  -0.00778192874869233\n",
      "Weight of sp:  16 is:  -0.007638712531964466\n",
      "Weight of sp:  0 is:  -0.007198918334589287\n",
      "Weight of sp:  33 is:  -0.007064233537291961\n",
      "Weight of sp:  15 is:  -0.00666814572626106\n",
      "Weight of sp:  36 is:  -0.00653291485653541\n",
      "Weight of sp:  31 is:  -0.004668513413829819\n",
      "Weight of sp:  34 is:  -0.0041996332559363515\n",
      "Weight of sp:  9 is:  -0.0022475001157123494\n",
      "Weight of sp:  17 is:  0.002161094220940785\n",
      "Weight of sp:  44 is:  -0.001574224334943466\n",
      "Weight of sp:  28 is:  -0.0011515802534985266\n",
      "Weight of sp:  3 is:  -1.4147804387464608e-05\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "\n",
    "annotated_image = la.annotate_image_parts(image, model, OUTPUT_DIR, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Bild nun annotiert ist (als Annotation wurden auch die Gewichte von LIME für die einzelnen Elemente gefunden), können wir nun die wichtigsten __K__ Bildelemente mit der Funktion __find_important_parts__ finden. Anschließend können Sie auch die Relationen zwischen den Bildteilen mit der Funktion __find_spatial_relations__ finden lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min weight: 0.047958463546489916\n",
      "Currently at superpixel 46\n",
      "Currently at superpixel 62\n",
      "Currently at superpixel 54\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "\n",
    "important_superpixels = la.find_important_parts(annotated_image, K)\n",
    "relations = la.find_spatial_relations(important_superpixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Liste, welche von der Funktion zum Finden von Relationen zurückgegeben wurde, beinhaltet Objekte vom Typ __Relation__. Hier geben wir nun beispielhaft die Informationen der ersten Relation aus. Natürlich müssen Sie den Namen der Liste an Ihre Implementation anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bottom_of\n",
      "Start: 62\n",
      "To: 46\n"
     ]
    }
   ],
   "source": [
    "print(\"Name:\", relations[0].name)\n",
    "print(\"Start:\", relations[0].start)\n",
    "print(\"To:\", relations[0].to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Name beschreibt das Prädikat der räumlichen Relation. Die weiteren Informationen beschreiben die Indices der Start- und Zielelemente der Relation innerhalb des Bildes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir das perturbierte Datenset für LIME-Aleph generieren lassen. Benutzen Sie hierzu die Funktion __perturb_instance__ mit den erforderlichen Parametern. Lassen Sie sich auch ausgeben, wie viele Instanzen im neuen Datenset sind (Es wird eine Liste mit Instanzen zurückgegeben)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of perturbed instances: 11\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "perturbed_dataset = la.perturb_instance(annotated_image, relations, model, T)\n",
    "print(\"Number of perturbed instances:\", len(perturbed_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ILP-Framework Aleph benötigt mehrere Hilfsdateien, die mit der Funktion __write_aleph_files__ erzeugt werden. Rufen Sie diese Funktion auf. Es sollen alle räumlichen Relationen verwendet werden! Zur Verfügung stehen folgende Relationen: *left_of*, *right_of*, *top_of*, *bottom_of*, *on*, *under*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the input files for Aleph...\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "used_relations = None # 'None' if you want to allow all relations, otherwise list with following possibilities: [\"left_of\", \"right_of\", \"top_of\", \"bottom_of\", \"on\", \"under\"]\n",
    "la.write_aleph_files(annotated_image, perturbed_dataset, used_relations, OUTPUT_DIR, NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schlussendlich muss nun der Induktionsprozess von Aleph angestoßen werden. Dieser Schritt (mit der Funktion __run_aleph__) gibt auch die gefundenen Erklärungen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "The explanation was saved to ../output/\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "la.run_aleph(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
