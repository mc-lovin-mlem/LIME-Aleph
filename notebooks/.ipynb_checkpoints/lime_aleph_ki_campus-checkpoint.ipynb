{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME-Aleph\n",
    "2021-02-17\n",
    "\n",
    "Johannes Rabold (Universität Bamberg) und Stephan Scheele (Fraunhofer IIS)\n",
    "\n",
    "Adaptiert für den KI-Campus Kurs XAI4Ing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontakt\n",
    "Über das git Repository von LIME-Aleph oder https://www.uni-bamberg.de/en/cogsys/team/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lizenz\n",
    "Diese Arbeit ist lizenziert unter [BSD-2-Clause Lizenz](https://opensource.org/licenses/BSD-2-Clause)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Willkommen zum Beispiel für das Modul __LIME-Aleph__ im KI-Campus. Hier werden Sie den typischen Ablauf zum Finden einer symbolischen Erklärung für Black-Box Netzwerke mithilfe der LIME-Aleph Bibliothek beschreiben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_J. Rabold, H. Deininger, M. Siebers, U. Schmid:. (2019)._ [„Enriching Visual with Verbal Explanations for Relational Concepts – Combining LIME with Aleph“](https://doi.org/10.1007/978-3-030-43823-4_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hinweis__: Bitte setzen Sie das Notebook zurück wenn sie dieses ein zweites mal ausführen möchten. \n",
    "    Im Menü über __(Kernel > Restart)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst importieren wir die benötigten Bibliotheken und definieren einige Parameter. \n",
    "Im Beispiel nutzen wir ein Bild zur Klassifikation und ein bereits bestehendes und vortrainiertes Modell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "from skimage.util import img_as_float32\n",
    "from skimage.transform import resize\n",
    "from scripts.train_model import own_rel\n",
    "from skimage import io\n",
    "from skimage.io import imshow, show, imsave\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import shutil\n",
    "from graphviz import Digraph\n",
    "\n",
    "import lime_aleph.lime_aleph as la\n",
    "\n",
    "IMAGE_FILE = \"pos9000.png\" # The path to the image file to be classified by the black-box\n",
    "MODEL = \"../models_to_explain/model_tower.h5\" # The path to the pre-trained model\n",
    "K = 4 # Number of relevant pixels\n",
    "N = 1000 # The sample size for LIME\n",
    "OUTPUT_DIR = \"../output/\" # A path for a directory to save intermediate and output data\n",
    "T = 0.8 # The threshold for the binary classifier for when an example is classified as 'positive'\n",
    "NOISE = 10 # The allowed false-positive rate for Aleph in percent.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sollte es noch temporäre Daten aus früheren Durchläufen geben, sollen diese nun gelöscht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden das Bild und das vortrainierte Modell in den Speicher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img_as_float32(io.imread(IMAGE_FILE))\n",
    "image = resize(image, (own_rel.IMAGE_SIZE, own_rel.IMAGE_SIZE), anti_aliasing=True)\n",
    "\n",
    "model = own_rel.own_rel()\n",
    "model.load_weights(MODEL)\n",
    "\n",
    "io.imshow(image)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen einen Turm bestehend aus drei Bauklötzen der Farben Cyan, Grün und Blau sowie den Hintergrund mit der Farbe Rot. Der blaue Bauklotz bildet das Fundament des Turms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation des Bildes\n",
    "Im Folgenden werden die im Bild vorhandenen Elemente automatisch annotiert. \n",
    "Dazu verwenden wir die Funktion __annotate_image_parts__ aus dem Package __lime_aleph__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_image = la.annotate_image_parts(image, model, OUTPUT_DIR, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion findet auch die Bildteile, die laut LIME für die Klasse am wichtigsten sind. Wir können diese Teile sowohl für die positive als auch für die negative Klasse visualisieren.\n",
    "Das erste Bild zeigt die wichtigsten Features im Bild für die Klasse __Turm__, hier werden die drei Bauklötze erkannt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_class = 1\n",
    "\n",
    "temp, mask = annotated_image.explanation.get_image_and_mask(true_class, positive_only=True, num_features=K, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das zweite Bild zeigt dazu konträre Superpixel zur Klasse Turm, d.h. es werden Bereiche der Hintergrundfarbe identifiziert, die voneinander entfernt sind und nicht alle einander berühren wie die Bauklötze des Turms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_class = 0\n",
    "\n",
    "temp, mask = annotated_image.explanation.get_image_and_mask(negative_class, positive_only=True, num_features=K, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Bild nun annotiert ist, wollen wir nun die wichtigsten __K (=4)__ Bildelemente mit der Funktion __find_important_parts__ finden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "important_superpixels, labeled_image = la.find_important_parts(annotated_image, K)\n",
    "\n",
    "plt.imshow(labeled_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_superpixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Bild werden den Bereichen Zahlen d.h. Klassenidentifier zugeordnet. \n",
    "Anschließend können wir die Relationen zwischen diesen Bildteilen mit der Funktion __find_spatial_relations__ identifizieren und als Graph extrahieren. \n",
    "Dies geschieht über die Funktion __find_spatial_relations__, angewendet auf die wichtigen Bildteile (Objekt __important_superpixels__). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations, graph = la.find_spatial_relations(important_superpixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Graph können wir nun mittels Graphviz visualisieren. \n",
    "Dieser beschreibt anhand binärer Relationen die topologischen Abhängigkeiten zwischen den einzelnen Objektklassen.\n",
    "Die Kanten und deren Label repräsentieren die räumlichen Relationen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Relationen können wir auch textuell ausgeben lassen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rel in relations:\n",
    "    print(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erzeugung perturbierter Instanzen\n",
    "Nun lassen wir uns das perturbierte Datenset für LIME-Aleph generieren. Hierzu nutzen wir die Funktion __perturb_instance__ mit den erforderlichen Parametern. Dabei geben wir im Anschluss auch aus, wie viele Instanzen im neuen Datenset enthalten sind (Liste der Instanzen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_dataset = la.perturb_instance(annotated_image, relations, model, T)\n",
    "print(\"Number of perturbed instances:\", len(perturbed_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Instanzen können wir inspizieren und sehen dabei, dass positive und negative Beispiele generiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in perturbed_dataset:\n",
    "    print(ex)\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt visualisieren wir noch alle Instanzen aus dem perturbierten Datenset. Erst die positiven Instanzen für die die Klasse Turm weiterhin gilt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ex in perturbed_dataset:\n",
    "    if ex.positive:\n",
    "        plt.imshow(ex.labeled_image)\n",
    "        plt.show()\n",
    "        display(ex.graph)\n",
    "        print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und nun alle negativen Instanzen, für die die Klasse Turm nicht mehr gilt.\n",
    "Dies ist per Definition der Fall wenn der __dunkelblaue Bauklotz__ __nicht__ mehr das __Fundament bildet__ d.h. ganz unten platziert ist und/oder die __Bauklötze räumlich voneinander getrennt__ sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ex in perturbed_dataset:\n",
    "    if not ex.positive:\n",
    "        plt.imshow(ex.labeled_image)\n",
    "        plt.show()\n",
    "        display(ex.graph)\n",
    "        print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ILP-Framework Aleph benötigt mehrere Hilfsdateien, die mit der Funktion __write_aleph_files__ erzeugt werden. \n",
    "Es sollen alle räumlichen Relationen verwendet werden! Zur Verfügung stehen folgende Relationen: *left_of*, *right_of*, *top_of*, *bottom_of*, *on*, *under*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_relations = None # 'None' if you want to allow all relations, otherwise list with following possibilities: [\"left_of\", \"right_of\", \"top_of\", \"bottom_of\", \"on\", \"under\"]\n",
    "la.write_aleph_files(annotated_image, perturbed_dataset, used_relations, OUTPUT_DIR, NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schlussendlich muss nun der Induktionsprozess von Aleph angestoßen werden. Dieser Schritt (mit der Funktion __run_aleph__) gibt auch die gefundene Erklärung zurück:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "la.run_aleph(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Erklärung in Form von Regeln findet sich nun im angegebenen Ordner in der Datei *explanation.txt*. \n",
    "Wir lesen diese Datei aus und stellen die Regel in Form einer Prolog-Regel dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR + \"explanation.txt\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "Die Interpretation der Regel wollen wir im Folgenden an einem Beispiel darstellen. \n",
    "\n",
    "true_class(A) :-\n",
    "   contains(B,A), has_color(B,mediumblue), contains(C,A), has_color(C,lime), \n",
    "   top_of_in_ex(C,B,A).\n",
    "\n",
    "Die Lesart der Regel ist wie folgt. Das Prädikat __true_class(A)__ entspricht der Aussage _\"A ist ein Turm\"_ und gilt wenn folgende Aussagen logisch wahr sind:\n",
    "* der Turm A enthällt einen Bauklotz B, \n",
    "* Bauklotz B hat die Farbe Blau (__mediumblue__),\n",
    "* der Turm A enthält auch einen Bauklotz C,\n",
    "* der Bauklotz C hat die Farbe Grün (__lime__), und \n",
    "* C befindet sich in Relation __top_of__ zu B, d.h. C steht räumlich auf B.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
