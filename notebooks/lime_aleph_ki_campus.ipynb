{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME-Aleph\n",
    "\n",
    "### KI-Campus Aufgabe\n",
    "\n",
    "Willkommen zum Arbeitsauftrag für das Modul __LIME-Aleph__ im KI-Campus. Hier werden Sie den typischen Ablauf zum Finden einer symbolischen Erklärung für Black-Box Netzwerke mithilfe der LIME-Aleph Bibliothek Stück für Stück erarbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen zunächst mal die nötigen Bibliotheken importieren und einige nutzerdefinierbare Parameter erzeugen. Eine zu klassifizierende Bilddatei sowie ein vortrainiertes Modell sind schon vorhanden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import img_as_float32\n",
    "from skimage.transform import resize\n",
    "from train_model import own_rel\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "from skimage import io\n",
    "from skimage.io import imshow, show, imsave\n",
    "import shutil\n",
    "\n",
    "import lime_aleph as la\n",
    "\n",
    "NOTEBOOKPATH = \"\" #\n",
    "IMAGE_FILE = \"./pos9000.png\" # The path to the image file to be classified by the black-box\n",
    "MODEL = \"../models_to_explain/model_tower.h5\" # The path to the pre-trained model\n",
    "K = 3 # The number of important superpixels to be used for perturbation\n",
    "N = 1000 # The sample size for LIME\n",
    "OUTPUT_DIR = \"../output/\" # A path for a directory to save intermediate and output data\n",
    "T = 0.8 # The threshold for the binary classifier for when an example is classified as 'positive'\n",
    "NOISE = 10 # The allowed false-positive rate for Aleph in percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sollte es noch temporäre Daten aus früheren Durchläufen geben, sollen diese nun gelöscht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir das Bild und das vortrainierte Modell in den Speicher laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: '/Users/sees/Documents/IIS/Code/LIME-Aleph/output/pos9000.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-7822c04663b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_as_float32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mown_rel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mown_rel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manti_aliasing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mown_rel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mown_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lime-aleph/lib/python3.8/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lime-aleph/lib/python3.8/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                                (plugin, kind))\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lime-aleph/lib/python3.8/site-packages/skimage/io/_plugins/imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/lime-aleph/lib/python3.8/site-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Get reader and read first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lime-aleph/lib/python3.8/site-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Create request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# Get format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lime-aleph/lib/python3.8/site-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/lime-aleph/lib/python3.8/site-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file: '/Users/sees/Documents/IIS/Code/LIME-Aleph/output/pos9000.png'"
     ]
    }
   ],
   "source": [
    "image = img_as_float32(io.imread(IMAGE_FILE))\n",
    "image = resize(image, (own_rel.IMAGE_SIZE, own_rel.IMAGE_SIZE), anti_aliasing=True)\n",
    "\n",
    "model = own_rel.own_rel()\n",
    "model.load_weights(MODEL)\n",
    "\n",
    "io.imshow(image)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Schritt soll nun sein, die im Bild vorhandenen Elemente automatisch zu annotieren. Benutzen Sie hierfür die Funktion __annotate_image_parts__ aus dem bereits importierten __lime_aleph__ package mit den benötigten Parametern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LIME...\n",
      "True class of the image is:  1\n",
      "Negative estimator: 0.023822417\n",
      "Positive estimator: 0.9761776\n",
      "Starting the explanation generation process. This may take several minutes. Seriously. Grab a cup of coffee.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sees/.virtualenvs/lime-aleph/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bf9282c95f44fda0686b392e991d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept -0.23463660278704856\n",
      "Prediction_local [0.33396255]\n",
      "Right: 0.023822423\n",
      "Intercept 1.2346365973356541\n",
      "Prediction_local [0.66603746]\n",
      "Right: 0.9761776\n",
      "Elapsed time: 3.3394289016723633\n",
      "Number of superpixels: 64\n",
      "Annotating the superpixels...\n",
      "Weight of sp:  44 is:  0.1147492499928618\n",
      "Weight of sp:  60 is:  0.10612414946491816\n",
      "Weight of sp:  52 is:  0.09594455199558823\n",
      "Weight of sp:  48 is:  -0.04471325524619362\n",
      "Weight of sp:  10 is:  -0.042861667219122955\n",
      "Weight of sp:  17 is:  -0.040696741842656266\n",
      "Weight of sp:  53 is:  -0.03873590512069972\n",
      "Weight of sp:  41 is:  -0.03511165823358521\n",
      "Weight of sp:  42 is:  -0.03406053637765858\n",
      "Weight of sp:  50 is:  -0.03400254754601082\n",
      "Weight of sp:  45 is:  -0.030626829520449806\n",
      "Weight of sp:  34 is:  -0.029723969870578664\n",
      "Weight of sp:  54 is:  -0.029345078687959124\n",
      "Weight of sp:  40 is:  -0.028135710391808816\n",
      "Weight of sp:  18 is:  -0.027948492920774188\n",
      "Weight of sp:  57 is:  -0.027840963429139852\n",
      "Weight of sp:  14 is:  -0.02731158867698219\n",
      "Weight of sp:  26 is:  -0.026444598137094606\n",
      "Weight of sp:  29 is:  -0.023221443456149727\n",
      "Weight of sp:  19 is:  -0.022857631716684274\n",
      "Weight of sp:  24 is:  -0.022384147722965424\n",
      "Weight of sp:  21 is:  -0.02079116619692924\n",
      "Weight of sp:  32 is:  -0.020558591506413596\n",
      "Weight of sp:  51 is:  -0.01937013494725799\n",
      "Weight of sp:  13 is:  -0.019003774909456757\n",
      "Weight of sp:  25 is:  -0.017606223451654072\n",
      "Weight of sp:  16 is:  -0.017155316581784794\n",
      "Weight of sp:  20 is:  -0.016189323753193072\n",
      "Weight of sp:  3 is:  -0.015508233251422242\n",
      "Weight of sp:  61 is:  -0.014289296743052664\n",
      "Weight of sp:  5 is:  -0.013702918445720238\n",
      "Weight of sp:  35 is:  -0.013107002559109298\n",
      "Weight of sp:  22 is:  -0.012777323425329567\n",
      "Weight of sp:  38 is:  -0.012703192378807699\n",
      "Weight of sp:  63 is:  -0.012279784409175576\n",
      "Weight of sp:  59 is:  -0.01182689300423197\n",
      "Weight of sp:  12 is:  -0.01155264636862703\n",
      "Weight of sp:  43 is:  -0.01129346976738766\n",
      "Weight of sp:  56 is:  -0.011291315590738116\n",
      "Weight of sp:  0 is:  -0.01121289459764548\n",
      "Weight of sp:  62 is:  0.011170610792208609\n",
      "Weight of sp:  36 is:  -0.01094104191082158\n",
      "Weight of sp:  58 is:  -0.010566066289578761\n",
      "Weight of sp:  9 is:  0.010385461644600212\n",
      "Weight of sp:  39 is:  -0.010300165401298383\n",
      "Weight of sp:  31 is:  -0.01024373122376159\n",
      "Weight of sp:  33 is:  -0.009758286010069091\n",
      "Weight of sp:  11 is:  0.009357377924471446\n",
      "Weight of sp:  46 is:  -0.008102240743687579\n",
      "Weight of sp:  47 is:  -0.007347385071166499\n",
      "Weight of sp:  6 is:  0.007228561857237114\n",
      "Weight of sp:  30 is:  -0.006434159396752457\n",
      "Weight of sp:  7 is:  -0.006393849279443414\n",
      "Weight of sp:  1 is:  -0.006198162412613521\n",
      "Weight of sp:  37 is:  0.005931856488638536\n",
      "Weight of sp:  8 is:  0.005629368095547148\n",
      "Weight of sp:  2 is:  0.005258256684988363\n",
      "Weight of sp:  49 is:  -0.002874932315596661\n",
      "Weight of sp:  23 is:  -0.0025444739263799825\n",
      "Weight of sp:  15 is:  -0.0007767081777320581\n",
      "Weight of sp:  28 is:  -0.0007561330606734118\n",
      "Weight of sp:  55 is:  0.0005562771451759953\n",
      "Weight of sp:  27 is:  0.00029049233318270446\n",
      "Weight of sp:  4 is:  0.0002542542737326466\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "\n",
    "annotated_image = la.annotate_image_parts(image, model, OUTPUT_DIR, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do it manually....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LIME...\n",
      "Negative estimator: 0.023822417\n",
      "Positive estimator: 0.9761776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sees/.virtualenvs/lime-aleph/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Running LIME...\")\n",
    "\n",
    "true_class = -1\n",
    "predictions = model.predict_proba(np.array([image])) #Probabilities\n",
    "predictions = np.squeeze(predictions) #to get rid of the third dimension\n",
    "true_class = np.argmax(predictions)\n",
    "\n",
    "# the softmax output for the two classes:\n",
    "print(\"Negative estimator:\", predictions[0])\n",
    "print(\"Positive estimator:\", predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the explanation generation process. This may take several minutes. Seriously. Grab a cup of coffee.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccb07642c044ab7b18b53d160312d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept -0.20537472337552654\n",
      "Prediction_local [0.32172826]\n",
      "Right: 0.023822423\n",
      "Intercept 1.205374721315089\n",
      "Prediction_local [0.67827174]\n",
      "Right: 0.9761776\n",
      "Elapsed time: 3.3644580841064453\n"
     ]
    }
   ],
   "source": [
    "annotated_image = la.Image()\n",
    "annotated_image.true_class = true_class\n",
    "\n",
    "annotated_image.original_image = image\n",
    "\n",
    "\n",
    "from lime import lime_image\n",
    "from sources.own_segmentation.scikit_image import SegmentationAlgorithm\n",
    "import time\n",
    "\n",
    "n_lime_samples=1000\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer(verbose=True)\n",
    "print('Starting the explanation generation process. This may take several minutes. Seriously. Grab a cup of coffee.')\n",
    "sf = SegmentationAlgorithm('quadratic', n_quads=8) # our own segmentation algorithm. A uniform grid.\n",
    "tmp = time.time()\n",
    "explanation = explainer.explain_instance(image, model.predict_proba, segmentation_fn=sf, top_labels=2, num_samples=n_lime_samples, hide_color=0)\n",
    "print(\"Elapsed time: \" + str(time.time() - tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x192f0ceb0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "\n",
    "temp, mask = annotated_image.explanation.get_image_and_mask(true_class, positive_only=True, num_features=3, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x192f9e730>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp, mask = explanation.get_image_and_mask(0, positive_only=True, num_features=3, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem das Bild nun annotiert ist (als Annotation wurden auch die Gewichte von LIME für die einzelnen Elemente gefunden), können wir nun die wichtigsten __K__ Bildelemente mit der Funktion __find_important_parts__ finden. Anschließend können Sie auch die Relationen zwischen den Bildteilen mit der Funktion __find_spatial_relations__ finden lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at superpixel 44\n",
      "Currently at superpixel 60\n",
      "Currently at superpixel 52\n",
      "Currently at superpixel 48\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "\n",
    "important_superpixels = la.find_important_parts(annotated_image, 4)\n",
    "relations = la.find_spatial_relations(important_superpixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'true_class': 1,\n",
       " 'original_image': array([[[0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         ...,\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ]],\n",
       " \n",
       "        [[0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         ...,\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ]],\n",
       " \n",
       "        [[0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         ...,\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         ...,\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ]],\n",
       " \n",
       "        [[0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         ...,\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ]],\n",
       " \n",
       "        [[0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         ...,\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ],\n",
       "         [0.5019608, 0.       , 0.       ]]], dtype=float32),\n",
       " 'superpixel_mask': array([[ 0,  0,  0, ...,  7,  7,  7],\n",
       "        [ 0,  0,  0, ...,  7,  7,  7],\n",
       "        [ 0,  0,  0, ...,  7,  7,  7],\n",
       "        ...,\n",
       "        [56, 56, 56, ..., 63, 63, 63],\n",
       "        [56, 56, 56, ..., 63, 63, 63],\n",
       "        [56, 56, 56, ..., 63, 63, 63]]),\n",
       " 'superpixels': [<lime_aleph.Superpixel at 0x192f1f6d0>,\n",
       "  <lime_aleph.Superpixel at 0x192d9f070>,\n",
       "  <lime_aleph.Superpixel at 0x192827130>,\n",
       "  <lime_aleph.Superpixel at 0x192ee4340>,\n",
       "  <lime_aleph.Superpixel at 0x192ee8910>,\n",
       "  <lime_aleph.Superpixel at 0x192ee4c70>,\n",
       "  <lime_aleph.Superpixel at 0x10a19e970>,\n",
       "  <lime_aleph.Superpixel at 0x192ee4220>,\n",
       "  <lime_aleph.Superpixel at 0x192823b80>,\n",
       "  <lime_aleph.Superpixel at 0x192d94c40>,\n",
       "  <lime_aleph.Superpixel at 0x192ee4e50>,\n",
       "  <lime_aleph.Superpixel at 0x192d94520>,\n",
       "  <lime_aleph.Superpixel at 0x192d94ac0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94ee0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94220>,\n",
       "  <lime_aleph.Superpixel at 0x192d94310>,\n",
       "  <lime_aleph.Superpixel at 0x192d942e0>,\n",
       "  <lime_aleph.Superpixel at 0x192d943d0>,\n",
       "  <lime_aleph.Superpixel at 0x192d942b0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94c70>,\n",
       "  <lime_aleph.Superpixel at 0x192d94f40>,\n",
       "  <lime_aleph.Superpixel at 0x192d949d0>,\n",
       "  <lime_aleph.Superpixel at 0x192d946a0>,\n",
       "  <lime_aleph.Superpixel at 0x192d949a0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94820>,\n",
       "  <lime_aleph.Superpixel at 0x192d948b0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94730>,\n",
       "  <lime_aleph.Superpixel at 0x192d94850>,\n",
       "  <lime_aleph.Superpixel at 0x192d94d60>,\n",
       "  <lime_aleph.Superpixel at 0x192d94e50>,\n",
       "  <lime_aleph.Superpixel at 0x192d94370>,\n",
       "  <lime_aleph.Superpixel at 0x192d945e0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94160>,\n",
       "  <lime_aleph.Superpixel at 0x192d94490>,\n",
       "  <lime_aleph.Superpixel at 0x192d94940>,\n",
       "  <lime_aleph.Superpixel at 0x192d94be0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94880>,\n",
       "  <lime_aleph.Superpixel at 0x192d94a90>,\n",
       "  <lime_aleph.Superpixel at 0x192e7d7c0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94100>,\n",
       "  <lime_aleph.Superpixel at 0x192d94dc0>,\n",
       "  <lime_aleph.Superpixel at 0x192d945b0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94c10>,\n",
       "  <lime_aleph.Superpixel at 0x192d94b20>,\n",
       "  <lime_aleph.Superpixel at 0x192e7db80>,\n",
       "  <lime_aleph.Superpixel at 0x192d944f0>,\n",
       "  <lime_aleph.Superpixel at 0x192e7dcd0>,\n",
       "  <lime_aleph.Superpixel at 0x192d94430>,\n",
       "  <lime_aleph.Superpixel at 0x192d94f10>,\n",
       "  <lime_aleph.Superpixel at 0x192d94790>,\n",
       "  <lime_aleph.Superpixel at 0x192d947c0>,\n",
       "  <lime_aleph.Superpixel at 0x192f44c40>,\n",
       "  <lime_aleph.Superpixel at 0x192f44a60>,\n",
       "  <lime_aleph.Superpixel at 0x192ddb580>,\n",
       "  <lime_aleph.Superpixel at 0x192f44070>,\n",
       "  <lime_aleph.Superpixel at 0x192ddbf70>,\n",
       "  <lime_aleph.Superpixel at 0x192f02100>,\n",
       "  <lime_aleph.Superpixel at 0x192f44ca0>,\n",
       "  <lime_aleph.Superpixel at 0x192f02b80>,\n",
       "  <lime_aleph.Superpixel at 0x192f44be0>,\n",
       "  <lime_aleph.Superpixel at 0x192f028e0>,\n",
       "  <lime_aleph.Superpixel at 0x192f44160>,\n",
       "  <lime_aleph.Superpixel at 0x192f44f10>,\n",
       "  <lime_aleph.Superpixel at 0x192f02b50>]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_superpixels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 44,\n",
       " 'name': 'sp_44',\n",
       " 'color': 'darkturquoise',\n",
       " 'size': 16,\n",
       " 'lime_weight': 0.1147492499928618,\n",
       " 'x_coord': 17.5,\n",
       " 'y_coord': 21.5}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(important_superpixels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 60,\n",
       " 'name': 'sp_60',\n",
       " 'color': 'mediumblue',\n",
       " 'size': 16,\n",
       " 'lime_weight': 0.10612414946491816,\n",
       " 'x_coord': 17.5,\n",
       " 'y_coord': 29.5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(important_superpixels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 52,\n",
       " 'name': 'sp_52',\n",
       " 'color': 'lime',\n",
       " 'size': 16,\n",
       " 'lime_weight': 0.09594455199558823,\n",
       " 'x_coord': 17.5,\n",
       " 'y_coord': 25.5}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(important_superpixels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Liste, welche von der Funktion zum Finden von Relationen zurückgegeben wurde, beinhaltet Objekte vom Typ __Relation__. Hier geben wir nun beispielhaft die Informationen der ersten Relation aus. Natürlich müssen Sie den Namen der Liste an Ihre Implementation anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bottom_of\n",
      "Start: 60\n",
      "To: 44\n"
     ]
    }
   ],
   "source": [
    "print(\"Name:\", relations[0].name)\n",
    "print(\"Start:\", relations[0].start)\n",
    "print(\"To:\", relations[0].to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 --bottom_of--> 44\n",
      "52 --under--> 44\n",
      "52 --bottom_of--> 44\n",
      "48 --left_of--> 44\n",
      "44 --top_of--> 60\n",
      "52 --on--> 60\n",
      "52 --top_of--> 60\n",
      "48 --left_of--> 60\n",
      "44 --on--> 52\n",
      "44 --top_of--> 52\n",
      "60 --under--> 52\n",
      "60 --bottom_of--> 52\n",
      "48 --left_of--> 52\n",
      "44 --right_of--> 48\n",
      "60 --right_of--> 48\n",
      "52 --right_of--> 48\n"
     ]
    }
   ],
   "source": [
    "for idx in relations:\n",
    "    print(idx.start, \"--\"+idx.name+\"-->\", idx.to) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Name beschreibt das Prädikat der räumlichen Relation. Die weiteren Informationen beschreiben die Indices der Start- und Zielelemente der Relation innerhalb des Bildes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir das perturbierte Datenset für LIME-Aleph generieren lassen. Benutzen Sie hierzu die Funktion __perturb_instance__ mit den erforderlichen Parametern. Lassen Sie sich auch ausgeben, wie viele Instanzen im neuen Datenset sind (Es wird eine Liste mit Instanzen zurückgegeben)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of perturbed instances: 17\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "perturbed_dataset = la.perturb_instance(annotated_image, relations, model, T)\n",
    "print(\"Number of perturbed instances:\", len(perturbed_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 44, 'name': 'sp_44', 'color': 'darkturquoise', 'size': 16, 'lime_weight': 0.1147492499928618, 'x_coord': 17.5, 'y_coord': 21.5}\n",
      "{'id': 60, 'name': 'sp_60', 'color': 'mediumblue', 'size': 16, 'lime_weight': 0.10612414946491816, 'x_coord': 17.5, 'y_coord': 29.5}\n",
      "{'id': 52, 'name': 'sp_52', 'color': 'lime', 'size': 16, 'lime_weight': 0.09594455199558823, 'x_coord': 17.5, 'y_coord': 25.5}\n",
      "{'id': 48, 'name': 'sp_48', 'color': 'maroon', 'size': 16, 'lime_weight': -0.04471325524619362, 'x_coord': 1.5, 'y_coord': 25.5}\n",
      "{'id': 10, 'name': 'sp_10', 'color': 'maroon', 'size': 16, 'lime_weight': -0.042861667219122955, 'x_coord': 9.5, 'y_coord': 5.5}\n",
      "{'id': 17, 'name': 'sp_17', 'color': 'maroon', 'size': 16, 'lime_weight': -0.040696741842656266, 'x_coord': 5.5, 'y_coord': 9.5}\n",
      "{'id': 53, 'name': 'sp_53', 'color': 'maroon', 'size': 16, 'lime_weight': -0.03873590512069972, 'x_coord': 21.5, 'y_coord': 25.5}\n",
      "{'id': 41, 'name': 'sp_41', 'color': 'maroon', 'size': 16, 'lime_weight': -0.03511165823358521, 'x_coord': 5.5, 'y_coord': 21.5}\n",
      "{'id': 42, 'name': 'sp_42', 'color': 'maroon', 'size': 16, 'lime_weight': -0.03406053637765858, 'x_coord': 9.5, 'y_coord': 21.5}\n",
      "{'id': 50, 'name': 'sp_50', 'color': 'maroon', 'size': 16, 'lime_weight': -0.03400254754601082, 'x_coord': 9.5, 'y_coord': 25.5}\n",
      "{'id': 45, 'name': 'sp_45', 'color': 'maroon', 'size': 16, 'lime_weight': -0.030626829520449806, 'x_coord': 21.5, 'y_coord': 21.5}\n",
      "{'id': 34, 'name': 'sp_34', 'color': 'maroon', 'size': 16, 'lime_weight': -0.029723969870578664, 'x_coord': 9.5, 'y_coord': 17.5}\n",
      "{'id': 54, 'name': 'sp_54', 'color': 'maroon', 'size': 16, 'lime_weight': -0.029345078687959124, 'x_coord': 25.5, 'y_coord': 25.5}\n",
      "{'id': 40, 'name': 'sp_40', 'color': 'maroon', 'size': 16, 'lime_weight': -0.028135710391808816, 'x_coord': 1.5, 'y_coord': 21.5}\n",
      "{'id': 18, 'name': 'sp_18', 'color': 'maroon', 'size': 16, 'lime_weight': -0.027948492920774188, 'x_coord': 9.5, 'y_coord': 9.5}\n",
      "{'id': 57, 'name': 'sp_57', 'color': 'maroon', 'size': 16, 'lime_weight': -0.027840963429139852, 'x_coord': 5.5, 'y_coord': 29.5}\n",
      "{'id': 14, 'name': 'sp_14', 'color': 'maroon', 'size': 16, 'lime_weight': -0.02731158867698219, 'x_coord': 25.5, 'y_coord': 5.5}\n",
      "{'id': 26, 'name': 'sp_26', 'color': 'maroon', 'size': 16, 'lime_weight': -0.026444598137094606, 'x_coord': 9.5, 'y_coord': 13.5}\n",
      "{'id': 29, 'name': 'sp_29', 'color': 'maroon', 'size': 16, 'lime_weight': -0.023221443456149727, 'x_coord': 21.5, 'y_coord': 13.5}\n",
      "{'id': 19, 'name': 'sp_19', 'color': 'maroon', 'size': 16, 'lime_weight': -0.022857631716684274, 'x_coord': 13.5, 'y_coord': 9.5}\n",
      "{'id': 24, 'name': 'sp_24', 'color': 'maroon', 'size': 16, 'lime_weight': -0.022384147722965424, 'x_coord': 1.5, 'y_coord': 13.5}\n",
      "{'id': 21, 'name': 'sp_21', 'color': 'maroon', 'size': 16, 'lime_weight': -0.02079116619692924, 'x_coord': 21.5, 'y_coord': 9.5}\n",
      "{'id': 32, 'name': 'sp_32', 'color': 'maroon', 'size': 16, 'lime_weight': -0.020558591506413596, 'x_coord': 1.5, 'y_coord': 17.5}\n",
      "{'id': 51, 'name': 'sp_51', 'color': 'maroon', 'size': 16, 'lime_weight': -0.01937013494725799, 'x_coord': 13.5, 'y_coord': 25.5}\n",
      "{'id': 13, 'name': 'sp_13', 'color': 'maroon', 'size': 16, 'lime_weight': -0.019003774909456757, 'x_coord': 21.5, 'y_coord': 5.5}\n",
      "{'id': 25, 'name': 'sp_25', 'color': 'maroon', 'size': 16, 'lime_weight': -0.017606223451654072, 'x_coord': 5.5, 'y_coord': 13.5}\n",
      "{'id': 16, 'name': 'sp_16', 'color': 'maroon', 'size': 16, 'lime_weight': -0.017155316581784794, 'x_coord': 1.5, 'y_coord': 9.5}\n",
      "{'id': 20, 'name': 'sp_20', 'color': 'maroon', 'size': 16, 'lime_weight': -0.016189323753193072, 'x_coord': 17.5, 'y_coord': 9.5}\n",
      "{'id': 3, 'name': 'sp_3', 'color': 'maroon', 'size': 16, 'lime_weight': -0.015508233251422242, 'x_coord': 13.5, 'y_coord': 1.5}\n",
      "{'id': 61, 'name': 'sp_61', 'color': 'maroon', 'size': 16, 'lime_weight': -0.014289296743052664, 'x_coord': 21.5, 'y_coord': 29.5}\n",
      "{'id': 5, 'name': 'sp_5', 'color': 'maroon', 'size': 16, 'lime_weight': -0.013702918445720238, 'x_coord': 21.5, 'y_coord': 1.5}\n",
      "{'id': 35, 'name': 'sp_35', 'color': 'maroon', 'size': 16, 'lime_weight': -0.013107002559109298, 'x_coord': 13.5, 'y_coord': 17.5}\n",
      "{'id': 22, 'name': 'sp_22', 'color': 'maroon', 'size': 16, 'lime_weight': -0.012777323425329567, 'x_coord': 25.5, 'y_coord': 9.5}\n",
      "{'id': 38, 'name': 'sp_38', 'color': 'maroon', 'size': 16, 'lime_weight': -0.012703192378807699, 'x_coord': 25.5, 'y_coord': 17.5}\n",
      "{'id': 63, 'name': 'sp_63', 'color': 'maroon', 'size': 16, 'lime_weight': -0.012279784409175576, 'x_coord': 29.5, 'y_coord': 29.5}\n",
      "{'id': 59, 'name': 'sp_59', 'color': 'maroon', 'size': 16, 'lime_weight': -0.01182689300423197, 'x_coord': 13.5, 'y_coord': 29.5}\n",
      "{'id': 12, 'name': 'sp_12', 'color': 'maroon', 'size': 16, 'lime_weight': -0.01155264636862703, 'x_coord': 17.5, 'y_coord': 5.5}\n",
      "{'id': 43, 'name': 'sp_43', 'color': 'maroon', 'size': 16, 'lime_weight': -0.01129346976738766, 'x_coord': 13.5, 'y_coord': 21.5}\n",
      "{'id': 56, 'name': 'sp_56', 'color': 'maroon', 'size': 16, 'lime_weight': -0.011291315590738116, 'x_coord': 1.5, 'y_coord': 29.5}\n",
      "{'id': 0, 'name': 'sp_0', 'color': 'maroon', 'size': 16, 'lime_weight': -0.01121289459764548, 'x_coord': 1.5, 'y_coord': 1.5}\n",
      "{'id': 62, 'name': 'sp_62', 'color': 'maroon', 'size': 16, 'lime_weight': 0.011170610792208609, 'x_coord': 25.5, 'y_coord': 29.5}\n",
      "{'id': 36, 'name': 'sp_36', 'color': 'maroon', 'size': 16, 'lime_weight': -0.01094104191082158, 'x_coord': 17.5, 'y_coord': 17.5}\n",
      "{'id': 58, 'name': 'sp_58', 'color': 'maroon', 'size': 16, 'lime_weight': -0.010566066289578761, 'x_coord': 9.5, 'y_coord': 29.5}\n",
      "{'id': 9, 'name': 'sp_9', 'color': 'maroon', 'size': 16, 'lime_weight': 0.010385461644600212, 'x_coord': 5.5, 'y_coord': 5.5}\n",
      "{'id': 39, 'name': 'sp_39', 'color': 'maroon', 'size': 16, 'lime_weight': -0.010300165401298383, 'x_coord': 29.5, 'y_coord': 17.5}\n",
      "{'id': 31, 'name': 'sp_31', 'color': 'maroon', 'size': 16, 'lime_weight': -0.01024373122376159, 'x_coord': 29.5, 'y_coord': 13.5}\n",
      "{'id': 33, 'name': 'sp_33', 'color': 'maroon', 'size': 16, 'lime_weight': -0.009758286010069091, 'x_coord': 5.5, 'y_coord': 17.5}\n",
      "{'id': 11, 'name': 'sp_11', 'color': 'maroon', 'size': 16, 'lime_weight': 0.009357377924471446, 'x_coord': 13.5, 'y_coord': 5.5}\n",
      "{'id': 46, 'name': 'sp_46', 'color': 'maroon', 'size': 16, 'lime_weight': -0.008102240743687579, 'x_coord': 25.5, 'y_coord': 21.5}\n",
      "{'id': 47, 'name': 'sp_47', 'color': 'maroon', 'size': 16, 'lime_weight': -0.007347385071166499, 'x_coord': 29.5, 'y_coord': 21.5}\n",
      "{'id': 6, 'name': 'sp_6', 'color': 'maroon', 'size': 16, 'lime_weight': 0.007228561857237114, 'x_coord': 25.5, 'y_coord': 1.5}\n",
      "{'id': 30, 'name': 'sp_30', 'color': 'maroon', 'size': 16, 'lime_weight': -0.006434159396752457, 'x_coord': 25.5, 'y_coord': 13.5}\n",
      "{'id': 7, 'name': 'sp_7', 'color': 'maroon', 'size': 16, 'lime_weight': -0.006393849279443414, 'x_coord': 29.5, 'y_coord': 1.5}\n",
      "{'id': 1, 'name': 'sp_1', 'color': 'maroon', 'size': 16, 'lime_weight': -0.006198162412613521, 'x_coord': 5.5, 'y_coord': 1.5}\n",
      "{'id': 37, 'name': 'sp_37', 'color': 'maroon', 'size': 16, 'lime_weight': 0.005931856488638536, 'x_coord': 21.5, 'y_coord': 17.5}\n",
      "{'id': 8, 'name': 'sp_8', 'color': 'maroon', 'size': 16, 'lime_weight': 0.005629368095547148, 'x_coord': 1.5, 'y_coord': 5.5}\n",
      "{'id': 2, 'name': 'sp_2', 'color': 'maroon', 'size': 16, 'lime_weight': 0.005258256684988363, 'x_coord': 9.5, 'y_coord': 1.5}\n",
      "{'id': 49, 'name': 'sp_49', 'color': 'maroon', 'size': 16, 'lime_weight': -0.002874932315596661, 'x_coord': 5.5, 'y_coord': 25.5}\n",
      "{'id': 23, 'name': 'sp_23', 'color': 'maroon', 'size': 16, 'lime_weight': -0.0025444739263799825, 'x_coord': 29.5, 'y_coord': 9.5}\n",
      "{'id': 15, 'name': 'sp_15', 'color': 'maroon', 'size': 16, 'lime_weight': -0.0007767081777320581, 'x_coord': 29.5, 'y_coord': 5.5}\n",
      "{'id': 28, 'name': 'sp_28', 'color': 'maroon', 'size': 16, 'lime_weight': -0.0007561330606734118, 'x_coord': 17.5, 'y_coord': 13.5}\n",
      "{'id': 55, 'name': 'sp_55', 'color': 'maroon', 'size': 16, 'lime_weight': 0.0005562771451759953, 'x_coord': 29.5, 'y_coord': 25.5}\n",
      "{'id': 27, 'name': 'sp_27', 'color': 'maroon', 'size': 16, 'lime_weight': 0.00029049233318270446, 'x_coord': 13.5, 'y_coord': 13.5}\n",
      "{'id': 4, 'name': 'sp_4', 'color': 'maroon', 'size': 16, 'lime_weight': 0.0002542542737326466, 'x_coord': 17.5, 'y_coord': 1.5}\n"
     ]
    }
   ],
   "source": [
    "for idx in annotated_image.superpixels:\n",
    "    print(vars(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ILP-Framework Aleph benötigt mehrere Hilfsdateien, die mit der Funktion __write_aleph_files__ erzeugt werden. Rufen Sie diese Funktion auf. Es sollen alle räumlichen Relationen verwendet werden! Zur Verfügung stehen folgende Relationen: *left_of*, *right_of*, *top_of*, *bottom_of*, *on*, *under*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the input files for Aleph...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "used_relations = None # 'None' if you want to allow all relations, otherwise list with following possibilities: [\"left_of\", \"right_of\", \"top_of\", \"bottom_of\", \"on\", \"under\"]\n",
    "la.write_aleph_files(annotated_image, perturbed_dataset, used_relations, OUTPUT_DIR, NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schlussendlich muss nun der Induktionsprozess von Aleph angestoßen werden. Dieser Schritt (mit der Funktion __run_aleph__) gibt auch die gefundene Erklärung aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{}]\n",
      "[{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n",
      "[{}]\n",
      "[{}]\n",
      "[{}]\n",
      "The explanation was saved to '../output/explanation.txt'\n"
     ]
    }
   ],
   "source": [
    "#[SOLUTION]\n",
    "la.run_aleph(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Erklärung in Form von Regeln kann nun im angegebenen Ordner in der Datei *explanation.txt* gefunden und interpretiert werden. Wir lesen nun diese Datei aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_class(A) :-\n",
      "   contains(B,A), has_color(B,mediumblue), contains(C,A), has_color(C,lime), \n",
      "   top_of_in_ex(C,B,A).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIR + \"explanation.txt\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation \n",
    "...\n",
    "\n",
    "\n",
    "true_class(A) :-\n",
    "   contains(B,A), has_color(B,mediumblue), contains(C,A), has_color(C,lime), \n",
    "   top_of_in_ex(C,B,A).\n",
    "\n",
    "## Interpretierte Regel\n",
    "\n",
    "true_class(Turm) :-\n",
    "   contains(Blau,Turm), has_color(Blau,mediumblue), contains(Grün,Turm), has_color(Grün,lime), \n",
    "   top_of_in_ex(Grün,Blau,Turm).\n",
    "\n",
    "übersetzt in Satz\n",
    "Turm ist wahr wenn gilt: Der blaue Klotz gehört zu einem Turm, ... \n",
    "\n",
    "[Bild]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
